{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of messages containing 'camera': 118\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import tomllib\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wa_analyzer.model import TextClustering\n",
    "\n",
    "# Initialize clustering model\n",
    "clustering = TextClustering()\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# %%\n",
    "# --- Load configuration and data ---\n",
    "configfile = Path(\"../config.toml\").resolve()\n",
    "with configfile.open(\"rb\") as f:\n",
    "    config = tomllib.load(f)\n",
    "\n",
    "datafile = (Path(\"..\") / Path(config[\"processed\"]) / config[\"current\"]).resolve()\n",
    "if not datafile.exists():\n",
    "    logger.warning(\n",
    "        \"Datafile does not exist. First run src/preprocess.py, and check the timestamp!\"\n",
    "    )\n",
    "\n",
    "df = pd.read_parquet(datafile)\n",
    "\n",
    "# --- Filter for the word 'camera' in the 'message' column ---\n",
    "camera_df = df[df['message'].str.contains('camera', case=False, na=False)]\n",
    "print(f\"Number of messages containing 'camera': {len(camera_df)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# --- Count messages per author ---\n",
    "top_authors = df['author'].value_counts().head(40).index  # default = 20 of 5\n",
    "df_top = df[df['author'].isin(top_authors)].copy()\n",
    "authors = list(np.unique(df_top.author))\n",
    "print(f\"Number of top authors: {len(authors)}\")\n",
    "\n",
    "# %%\n",
    "# --- Helper to remove URLs ---\n",
    "def remove_url(text):\n",
    "    return re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "\n",
    "# %%\n",
    "# --- Create corpus of text chunks per author ---\n",
    "n = 3000       # chunk size\n",
    "min_parts = 2  # minimum chunks\n",
    "\n",
    "corpus = {}\n",
    "for author in authors:\n",
    "    subset = df_top[df_top.author == author].reset_index()\n",
    "    longseq = \" \".join(subset.message)\n",
    "    parts = [longseq[i:i+n] for i in range(0, len(longseq), n)]\n",
    "    parts = [remove_url(chunk) for chunk in parts]\n",
    "    parts = [re.sub(\" +\", \" \", chunk) for chunk in parts]\n",
    "    if len(parts) > min_parts:\n",
    "        corpus[author] = parts\n",
    "\n",
    "print(f\"\\nFinal corpus authors: {list(corpus.keys())}\")\n",
    "\n",
    "# %%\n",
    "# --- Feature extraction ---\n",
    "vectorizer = CountVectorizer(analyzer=\"char\", ngram_range=(3, 3))\n",
    "parts = [part for text in corpus.values() for part in text]\n",
    "X = vectorizer.fit_transform(parts)\n",
    "X = np.asarray(X.todense())\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "\n",
    "# %%\n",
    "# --- Load author metadata ---\n",
    "with open(\"nested_users5.json\", \"r\") as f:\n",
    "    nested_users = json.load(f)\n",
    "\n",
    "author_info_df = (\n",
    "    pd.DataFrame(nested_users)\n",
    "    .T\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'author'})\n",
    ")\n",
    "\n",
    "# %%\n",
    "# --- Prepare labels for clustering based on age ---\n",
    "wa_labels = [k for k, v in corpus.items() for _ in range(len(v))]\n",
    "age_map = author_info_df.set_index(\"author\")[\"Older_then_50\"].to_dict()\n",
    "\n",
    "# Encode labels and assign colors\n",
    "colors = []\n",
    "for author in wa_labels:\n",
    "    is_older = age_map.get(author, None)\n",
    "    \n",
    "    if is_older:\n",
    "        colors.append(\"tab:orange\")  # Color for older\n",
    "    else:\n",
    "        colors.append(\"tab:green\")    # Color for younger\n",
    "\n",
    "# %%\n",
    "# --- Run clustering ---\n",
    "clustering(\n",
    "    text=[part for text in corpus.values() for part in text],\n",
    "    k=500,\n",
    "    labels=None,  # We'll handle coloring manually\n",
    "    batch=False,\n",
    "    method=\"tSNE\" #\"PCA\"\n",
    ")\n",
    "\n",
    "# %%\n",
    "# --- Overlay points with manual colors ---\n",
    "# Extract the scatter points from the last clustering plot\n",
    "sc = plt.gca().collections[0]\n",
    "offsets = sc.get_offsets()\n",
    "\n",
    "plt.clf()  # Clear previous plot\n",
    "plt.scatter(offsets[:, 0], offsets[:, 1], c=colors, s=50, alpha=0.8)\n",
    "\n",
    "# Legend\n",
    "legend_elements = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='tab:orange', markersize=10, label='Older than 50'),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='tab:green', markersize=10, label='Younger than 50')\n",
    "]\n",
    "plt.legend(handles=legend_elements, title=\"Age\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.title(\"WhatsApp messages (highlighting Age)\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wa-analyzer (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
